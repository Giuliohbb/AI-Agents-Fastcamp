{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Instalação\n",
        "Instalação de bibliotecas"
      ],
      "metadata": {
        "id": "WpPhx0Py9Skn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalação do groq\n",
        "%pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RndSsGwEV76U",
        "outputId": "31ce6f2e-e70f-4350-f457-02c71c19ef76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RLt7IFVVibd"
      },
      "outputs": [],
      "source": [
        "#Impostação da Chave da API encontrada no groq\n",
        "import os\n",
        "os.environ['GROQ_API_KEY'] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definição do client e modelo llm a ser usado\n",
        "#O cliente serve para interagir com a API da Groq\n",
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(\n",
        "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\", #Conteúdo da mensagem a ser gerada pelo modelo\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-70b-8192\", #Definição do modelo escolhido(llama-70b-8192)\n",
        ")\n",
        "#Exibe a mensagem gerada pelo modelo\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQiUEUwZdoFz",
        "outputId": "9b05a950-9cdb-44c2-e004-211814ce5905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models are essential in natural language processing (NLP) and have numerous applications in various industries. Here are some reasons why fast language models are important:\n",
            "\n",
            "1. **Real-time Processing**: Fast language models enable real-time processing of vast amounts of text data, which is crucial in applications like chatbots, virtual assistants, and sentiment analysis. They can quickly analyze and respond to user inputs, providing a seamless user experience.\n",
            "2. **Efficient Resource Utilization**: Fast language models require fewer computational resources, which leads to significant cost savings and reduced environmental impact. They can run on lower-end hardware, making them more accessible to organizations with limited resources.\n",
            "3. **Scalability**: Fast language models can handle large volumes of text data, making them ideal for applications that involve processing massive datasets, such as language translation, text summarization, and information retrieval.\n",
            "4. **Low Latency**: Fast language models reduce latency, which is critical in applications like real-time language translation, speech recognition, and dialog systems. This enables users to receive rapid responses and ensures a more interactive experience.\n",
            "5. **Improved User Experience**: Fast language models enable applications to respond quickly to user input, leading to a more engaging and interactive experience. This is particularly important in applications like customer service chatbots, where timely responses are essential.\n",
            "6. **Enhanced Productivity**: Fast language models can automate many tasks, such as text summarization, sentiment analysis, and language translation, freeing up human resources for more complex and creative tasks.\n",
            "7. **Competitive Advantage**: Organizations that can process and analyze large amounts of text data quickly gain a competitive advantage in their respective industries. Fast language models provide a strategic edge in areas like customer service, marketing, and market research.\n",
            "8. **Research and Development**: Fast language models accelerate the development of new NLP applications and techniques, as researchers can quickly experiment and test new ideas.\n",
            "9. **Healthcare and Emergency Response**: Fast language models can be used in healthcare applications, such as quickly analyzing medical texts to identify patterns and make diagnoses. They can also be used in emergency response systems to analyze and respond to emergency calls.\n",
            "10. **Environmental Applications**: Fast language models can be used in environmental applications, such as analyzing climate data, tracking deforestation, and monitoring wildlife populations, supporting efforts to mitigate the impact of climate change.\n",
            "\n",
            "In summary, fast language models are essential for enabling efficient, scalable, and real-time processing of large text datasets, which is critical in various industries, including customer service, marketing, healthcare, and environmental applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definição dos defs do agente\n",
        "class Agent: #Agent é uma classe que representa um agente de IA interagindo com um modelo de linguagem.\n",
        "  def __init__(self, client, system):\n",
        "    self.client = client\n",
        "    self.system = system\n",
        "    self.messages = [] #Armazena mensagens anteriores para manter o contexto da conversa.\n",
        "    if self.system is not None: #Se um system_prompt for definido, ele será adicionado como mensagem do sistema\n",
        "      self.messages.append({\"role\": \"system\", \"content\": self.system})\n",
        "\n",
        "  def __call__(self, message=\"\"): #Permite chamar o agente diretamente\n",
        "    if message:\n",
        "        self.messages.append({\"role\": \"user\", \"content\": message}) #Adiciona a mensagem do usuário ao histórico\n",
        "    result = self.execute() #Chama o método execute()\n",
        "    self.messages.append({\"role\": \"assistant\", \"content\": result})\n",
        "    return result\n",
        "\n",
        "  def execute(self): #Executa a solicitação para o modelo de linguagem e retorna a resposta\n",
        "      completion = client.chat.completions.create(\n",
        "          messages=self.messages,\n",
        "          model=\"llama-3.3-70b-versatile\", #Usa o modelo \"llama3-70b-8192\" para responder com base no contexto armazenado\n",
        "      )\n",
        "      return completion.choices[0].message.content #Retorna a resposta"
      ],
      "metadata": {
        "id": "4jZFtjwoeXSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt para o agente React, utilizando alturas de torres\n",
        "#este prompt define o ciclo a ser seguido de Thought, Action, PAUSE, Observation\n",
        "#Define as ferramentas disponíveis para o agente\n",
        "#Da um exemplo para melhorar a compreensão do agente\n",
        "system_prompt = \"\"\"\n",
        "You run in a loop of Thought, Action, PAUSE, Observation.\n",
        "At the end of the loop you output an Answer.\n",
        "Use Thought to describe your reasoning about the question you have been asked.\n",
        "Use Action to run one of the actions available to you - then return PAUSE.\n",
        "Observation will be the result of running those actions.\n",
        "\n",
        "Your available actions are:\n",
        "\n",
        "get_tower_height:\n",
        "e.g. get_tower_height: Eiffel Tower\n",
        "Returns the height of the tower in meters.\n",
        "\n",
        "calculate:\n",
        "e.g. calculate: 2 * 330\n",
        "Runs a calculation and returns the result.\n",
        "\n",
        "Example session:\n",
        "\n",
        "Question: What is twice the height of the Eiffel Tower?\n",
        "Thought: I need to get the height of the Eiffel Tower.\n",
        "Action: get_tower_height: Eiffel Tower\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 330\n",
        "\n",
        "Thought: I need to multiply that height by 2.\n",
        "Action: calculate: 2 * 330\n",
        "PAUSE\n",
        "\n",
        "You will be called again with this:\n",
        "\n",
        "Observation: 660\n",
        "\n",
        "If you have the answer, output it as the Answer.\n",
        "\n",
        "Thought: I have the final answer.\n",
        "Answer: Twice the height of the Eiffel Tower is 660 meters.\n",
        "\n",
        "Now it's your turn:\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "RDZUwaDsjAcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define a função para realizar operações matemáticas\n",
        "def calculate(operation): #Operações realizadas por meio de eval()\n",
        "    return eval(operation)\n",
        "\n",
        "#define a função para retornar a altura de torres famosas em metros\n",
        "def get_tower_height(tower_name): #Retorna a altura de torres famosas em metros\n",
        "    tower_heights = {\n",
        "        \"eiffel tower\": 330,\n",
        "        \"burj khalifa\": 828,\n",
        "        \"empire state building\": 381,\n",
        "        \"tokyo tower\": 332,\n",
        "        \"cn tower\": 553\n",
        "    }\n",
        "    return tower_heights.get(tower_name.lower(), \"Height not found\")\n"
      ],
      "metadata": {
        "id": "pbSHeevKZ-fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#impostação do Agente com base na classe e definição de seu nome\n",
        "luana = Agent(client=client, system=system_prompt)"
      ],
      "metadata": {
        "id": "S23Fz978kIke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rodando por uma vez o agente para verificar se está funcionando\n",
        "result = luana(\"What is the height of eiffel tower times 5?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfBjdBzkmB7y",
        "outputId": "1015c618-59bc-45cd-a007-b8865c8fcfaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: To find the height of the Eiffel Tower times 5, I first need to get the height of the Eiffel Tower.\n",
            "Action: get_tower_height: Eiffel Tower\n",
            "PAUSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#criaçãodo loop que irá rodar o ciclo múltiplas vezes até alcançar uma resposta satisfatória\n",
        "#Permite automatizar o ciclo do agente, rodando múltiplas iterações\n",
        "#Identifica ações sugeridas pelo modelo e executa funções (calculate ou get_planet_mass)\n",
        "#Usa regex (re.findall()) para capturar a ação desejada pelo modelo\n",
        "import re\n",
        "\n",
        "def agent_loop(max_iterations, system, query):\n",
        "  agent = Agent(client=client, system=system_prompt)\n",
        "  tools = ['calculate', 'get_tower_height']\n",
        "  next_prompt = query\n",
        "  i = 0\n",
        "  while i < max_iterations:\n",
        "    i += 1\n",
        "    result = agent(next_prompt)\n",
        "    print(result)\n",
        "\n",
        "    if \"PAUSE\" in result and \"Action\" in result:\n",
        "      action = re.findall(r'Action: ([a-z_]+): (.+)', result, re.IGNORECASE)\n",
        "      choosen_tool = action[0][0]\n",
        "      arg = action[0][1]\n",
        "\n",
        "      if choosen_tool in tools:\n",
        "        result_tool = eval(f\"{choosen_tool}('{arg}')\")\n",
        "        next_prompt = f\"Observation: {result_tool}\"\n",
        "\n",
        "      else:\n",
        "        next_prompt = \"Observation: Tool not found\"\n",
        "\n",
        "      print(next_prompt)\n",
        "      continue\n",
        "\n",
        "    if \"Answer\" in result:\n",
        "      break"
      ],
      "metadata": {
        "id": "oqcfAC8-ICGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bfkwzjFT9Rh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rodando o loop com os parametros desejados e inserindo a query\n",
        "agent_loop(max_iterations=10, system=system_prompt, query=\"What is the height of burj khalifa plus the height of the tokyo tower, and all of it times 4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BDkL0E_ICZz",
        "outputId": "46ca461d-50fa-4e6d-d876-01c6e517ece3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: To solve this problem, I need to get the heights of the Burj Khalifa and the Tokyo Tower, add them together, and then multiply the result by 4. First, I'll get the height of the Burj Khalifa.\n",
            "\n",
            "Action: get_tower_height: Burj Khalifa\n",
            "PAUSE\n",
            "Observation: 828\n",
            "Thought: Now that I have the height of the Burj Khalifa, I need to get the height of the Tokyo Tower to add them together.\n",
            "\n",
            "Action: get_tower_height: Tokyo Tower\n",
            "PAUSE\n",
            "Observation: 332\n",
            "Thought: I now have the heights of both the Burj Khalifa (828 meters) and the Tokyo Tower (332 meters). I'll add these two heights together.\n",
            "\n",
            "Action: calculate: 828 + 332\n",
            "PAUSE\n",
            "Observation: 1160\n",
            "Thought: I now have the sum of the heights of the Burj Khalifa and the Tokyo Tower, which is 1160 meters. The next step is to multiply this sum by 4 to get the final result.\n",
            "\n",
            "Action: calculate: 1160 * 4\n",
            "PAUSE\n",
            "Observation: 4640\n",
            "Thought: I have now calculated the sum of the heights of the Burj Khalifa and the Tokyo Tower multiplied by 4, which equals 4640 meters. This is the final answer to the problem.\n",
            "\n",
            "Answer: The height of the Burj Khalifa plus the height of the Tokyo Tower, all multiplied by 4, is 4640 meters.\n"
          ]
        }
      ]
    }
  ]
}